{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KBV89vMewKWV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBhpIWM7qDiq",
        "outputId": "f24044e1-757c-4915-d3b6-12b1bf40a969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.16.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: python-docx, colorama\n",
            "Successfully installed colorama-0.4.6 python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "%pip install python-docx openai pydantic colorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zaBPmb2rOUw",
        "outputId": "8f418dec-d100-49b9-fd5e-0710db037180"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sk-or-v1-\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ------------------------------------------------------------------\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-or-v1-\"\n",
        "os.environ[\"OPENAI_BASE_URL\"] = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "print(os.environ[\"OPENAI_API_KEY\"])\n",
        "# Choose your model. For hackathons, Claude 3.5 Sonnet is amazing at code/structure.\n",
        "# OpenRouter Model ID: \"anthropic/claude-3.5-sonnet\" or \"openai/gpt-4o\"\n",
        "MODEL_NAME = \"tngtech/deepseek-r1t2-chimera:free\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DMcLxiHXviWW"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyslxtD7vj5v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Paste your actual key here\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqnqjKe7sGYh",
        "outputId": "2d4d9ef9-994f-43c4-ed5c-ff99e6671981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning deep structure of suitability_report.docx...\n",
            "✅ Found 0 variables (including headers/footers).\n"
          ]
        }
      ],
      "source": [
        "!python analyze_v2.py suitability_report.docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HflOLkOtz5H",
        "outputId": "35d90309-1a5a-44dd-c1ba-ceb31eed056b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing analyze.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile analyze.py\n",
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import re\n",
        "from docx import Document\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "MODEL = \"google/gemini-2.5-flash\"\n",
        "\n",
        "# Concurrency Limit\n",
        "MAX_CONCURRENT_REQUESTS = 10\n",
        "\n",
        "ANALYSIS_SYSTEM_PROMPT = \"\"\"\n",
        "You are a Financial Document Expert. Analyze the text segment to identify \"Template Variables\".\n",
        "\n",
        "LOOK FOR:\n",
        "1. **Client Details:** Name (e.g., Sarah), Age.\n",
        "2. **Financial Goals:** specific amounts (£500k), dates (3 years), rates (8%).\n",
        "3. **Line Items:** If the text is a row in a table (e.g. \"Business Capital | £750,000\"), extract the values.\n",
        "\n",
        "OUTPUT JSON FORMAT:\n",
        "{\n",
        "  \"variables\": [\n",
        "    { \"original_text\": \"Sarah\", \"suggested_tag\": \"client_first_name\", \"type\": \"text\" },\n",
        "    { \"original_text\": \"£750,000\", \"suggested_tag\": \"asset_business_value\", \"type\": \"money\" }\n",
        "  ]\n",
        "}\n",
        "If no variables found, return { \"variables\": [] }\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. HELPER FUNCTIONS\n",
        "# -----------------------------------------------------------------------------\n",
        "def clean_json_response(content):\n",
        "    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)\n",
        "    content = re.sub(r'```json\\s*', '', content)\n",
        "    content = re.sub(r'```\\s*', '', content)\n",
        "    return content.strip()\n",
        "\n",
        "async def analyze_segment(sem, client, text, context_label, metadata):\n",
        "    \"\"\"\n",
        "    Analyzes a text segment (paragraph or table row).\n",
        "    \"\"\"\n",
        "    async with sem:\n",
        "        try:\n",
        "            # We provide context to the LLM (\"This is a Table Row\")\n",
        "            prompt = f\"Context: {context_label}\\nAnalyze this text: '{text}'\"\n",
        "\n",
        "            completion = await client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": ANALYSIS_SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            cleaned = clean_json_response(completion.choices[0].message.content)\n",
        "\n",
        "            try:\n",
        "                data = json.loads(cleaned)\n",
        "                variables = data.get(\"variables\", [])\n",
        "            except json.JSONDecodeError:\n",
        "                start = cleaned.find('{')\n",
        "                end = cleaned.rfind('}') + 1\n",
        "                if start != -1 and end != -1:\n",
        "                    variables = json.loads(cleaned[start:end]).get(\"variables\", [])\n",
        "                else:\n",
        "                    variables = []\n",
        "\n",
        "            # Enrich with location metadata\n",
        "            for v in variables:\n",
        "                v.update(metadata)\n",
        "\n",
        "            return variables\n",
        "\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. MAIN LOGIC\n",
        "# -----------------------------------------------------------------------------\n",
        "async def analyze_document(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return\n",
        "\n",
        "    doc = Document(file_path)\n",
        "    client = AsyncOpenAI(\n",
        "        api_key=API_KEY,\n",
        "        base_url=BASE_URL,\n",
        "        default_headers={\"HTTP-Referer\": \"https://colab.research.google.com\"}\n",
        "    )\n",
        "\n",
        "    print(f\"Analyzing {file_path}...\")\n",
        "    tasks = []\n",
        "    sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "\n",
        "    # --- A. SCAN PARAGRAPHS (With Context Window) ---\n",
        "    print(\"Scanning Paragraphs...\")\n",
        "    prev_text = \"\"\n",
        "    for i, para in enumerate(doc.paragraphs):\n",
        "        text = para.text.strip()\n",
        "\n",
        "        # Lower threshold to catch \"Sarah\" or \"Age 55\"\n",
        "        if len(text) < 3:\n",
        "            continue\n",
        "\n",
        "        # Create a \"Context Window\" (Previous Para + Current Para)\n",
        "        # This helps the AI know that \"Sarah\" is a Name.\n",
        "        combined_input = f\"Previous Line: {prev_text}\\nCurrent Line: {text}\"\n",
        "\n",
        "        tasks.append(analyze_segment(\n",
        "            sem, client, combined_input, \"Paragraph\",\n",
        "            {\"location_type\": \"paragraph\", \"index\": i}\n",
        "        ))\n",
        "\n",
        "        prev_text = text if len(text) > 5 else prev_text\n",
        "\n",
        "    # --- B. SCAN TABLES (Crucial for Financial Docs) ---\n",
        "    print(f\"Scanning {len(doc.tables)} Tables...\")\n",
        "    for t_idx, table in enumerate(doc.tables):\n",
        "        for r_idx, row in enumerate(table.rows):\n",
        "            # Convert row to a CSV-like string for the LLM\n",
        "            # e.g., \"Business Capital | Sarah | £750,000 | 15%\"\n",
        "            cells = [c.text.strip() for c in row.cells if c.text.strip()]\n",
        "            if not cells: continue\n",
        "\n",
        "            row_text = \" | \".join(cells)\n",
        "\n",
        "            # Skip header rows (heuristic: usually first row)\n",
        "            if r_idx == 0 and len(table.rows) > 1:\n",
        "                # Optional: You can choose to skip or scan headers.\n",
        "                # Scaning headers might help context but usually isn't data.\n",
        "                pass\n",
        "\n",
        "            tasks.append(analyze_segment(\n",
        "                sem, client, row_text, f\"Table {t_idx} Row {r_idx}\",\n",
        "                {\"location_type\": \"table\", \"table_index\": t_idx, \"row_index\": r_idx}\n",
        "            ))\n",
        "\n",
        "    # --- C. EXECUTE & SAVE ---\n",
        "    print(f\"Processing {len(tasks)} segments...\")\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    all_detections = [item for sublist in results for item in sublist]\n",
        "\n",
        "    out_file = file_path.replace(\".docx\", \"_analysis.json\")\n",
        "    with open(out_file, \"w\") as f:\n",
        "        json.dump(all_detections, f, indent=2)\n",
        "\n",
        "    print(f\"\\n✅ Found {len(all_detections)} variables (Clients, Assets, Liabilities).\")\n",
        "    print(f\"Saved to {out_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    asyncio.run(analyze_document(sys.argv[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIFvRYy6Qdw1",
        "outputId": "f1024332-e880-4ef2-b98c-649f529420de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting injection.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile injection.py\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "from docx import Document\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normalizes text to handle smart quotes and non-breaking spaces.\"\"\"\n",
        "    if not text: return \"\"\n",
        "    return text.replace('\\u00a0', ' ').replace('’', \"'\").replace('“', '\"').replace('”', '\"').strip()\n",
        "\n",
        "def inject_tags(docx_path, json_path):\n",
        "    if not os.path.exists(docx_path):\n",
        "        print(f\"{RED}Error: Document not found at {docx_path}{RESET}\")\n",
        "        return\n",
        "    if not os.path.exists(json_path):\n",
        "        print(f\"{RED}Error: JSON analysis not found at {json_path}{RESET}\")\n",
        "        return\n",
        "\n",
        "    doc = Document(docx_path)\n",
        "    with open(json_path, \"r\") as f:\n",
        "        variables = json.load(f)\n",
        "\n",
        "    print(f\"Injecting {len(variables)} tags into {docx_path}...\")\n",
        "    success_count = 0\n",
        "\n",
        "    for var in variables:\n",
        "        original_text = var.get('original_text')\n",
        "        tag_name = var.get('suggested_tag')\n",
        "        loc_type = var.get('location_type', 'paragraph')\n",
        "        target_index = var.get('index')\n",
        "\n",
        "        if not original_text or not tag_name:\n",
        "            continue\n",
        "\n",
        "        # Create the Tag\n",
        "        new_tag = f\"{{{{ {tag_name} }}}}\"\n",
        "\n",
        "        # -------------------------------------------------------\n",
        "        # STRATEGY 1: PARAGRAPH (With Search Window)\n",
        "        # -------------------------------------------------------\n",
        "        if loc_type == 'paragraph':\n",
        "            found = False\n",
        "\n",
        "            # Check the exact index first\n",
        "            if target_index < len(doc.paragraphs):\n",
        "                para = doc.paragraphs[target_index]\n",
        "                if smart_replace(para, original_text, new_tag):\n",
        "                    success_count += 1\n",
        "                    found = True\n",
        "\n",
        "            # If not found, check neighbors (Fuzzy Search)\n",
        "            # This handles cases where paragraph counts shift slightly\n",
        "            if not found:\n",
        "                start = max(0, target_index - 2)\n",
        "                end = min(len(doc.paragraphs), target_index + 3)\n",
        "                for i in range(start, end):\n",
        "                    if i == target_index: continue # Already checked\n",
        "                    if smart_replace(doc.paragraphs[i], original_text, new_tag):\n",
        "                        print(f\"{YELLOW}  -> Auto-corrected index from {target_index} to {i}{RESET}\")\n",
        "                        success_count += 1\n",
        "                        found = True\n",
        "                        break\n",
        "\n",
        "            if not found:\n",
        "                # DEBUG OUTPUT: Show user what went wrong\n",
        "                print(f\"{RED}Failed to find '{original_text}' near Para {target_index}{RESET}\")\n",
        "                if target_index < len(doc.paragraphs):\n",
        "                    print(f\"   Context in Doc: '{doc.paragraphs[target_index].text[:50]}...'\")\n",
        "\n",
        "        # -------------------------------------------------------\n",
        "        # STRATEGY 2: TABLE INJECTION\n",
        "        # -------------------------------------------------------\n",
        "        elif loc_type == 'table':\n",
        "            try:\n",
        "                table_idx = var.get('table_index')\n",
        "                row_idx = var.get('row_index')\n",
        "                if table_idx is not None and row_idx is not None:\n",
        "                    row = doc.tables[table_idx].rows[row_idx]\n",
        "                    for cell in row.cells:\n",
        "                        for p in cell.paragraphs:\n",
        "                            if smart_replace(p, original_text, new_tag):\n",
        "                                success_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"{RED}Table error: {e}{RESET}\")\n",
        "\n",
        "    output_path = docx_path.replace(\".docx\", \"_tagged.docx\")\n",
        "    doc.save(output_path)\n",
        "    print(f\"\\n{GREEN}Success! Injected {success_count}/{len(variables)} tags.{RESET}\")\n",
        "    print(f\"Saved to: {output_path}\")\n",
        "\n",
        "def smart_replace(paragraph, search_text, replace_text):\n",
        "    \"\"\"\n",
        "    Attempts to replace text handling 'runs' and 'normalization'.\n",
        "    \"\"\"\n",
        "    # 1. Normalize both sides for comparison\n",
        "    clean_para = normalize_text(paragraph.text)\n",
        "    clean_search = normalize_text(search_text)\n",
        "\n",
        "    # 2. Check if text is even present\n",
        "    if clean_search not in clean_para:\n",
        "        return False\n",
        "\n",
        "    # 3. Try Run Replacement (Preserves Bold/Color)\n",
        "    for run in paragraph.runs:\n",
        "        if search_text in run.text:\n",
        "            run.text = run.text.replace(search_text, replace_text)\n",
        "            return True\n",
        "\n",
        "    # 4. Fallback: Direct Replace (Might lose some styling, but ensures tag is inserted)\n",
        "    # We use the raw text replace here\n",
        "    if search_text in paragraph.text:\n",
        "        paragraph.text = paragraph.text.replace(search_text, replace_text)\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 3:\n",
        "        print(\"Usage: python injection.py <doc.docx> <analysis.json>\")\n",
        "    else:\n",
        "        inject_tags(sys.argv[1], sys.argv[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6_aPA3eQyl_",
        "outputId": "8357062d-83a3-468b-f340-60de87c8848d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Injecting 45 tags into suitability_report.docx...\n",
            "\u001b[91mFailed to find '£500,000' near Para 29\u001b[0m\n",
            "   Context in Doc: '   \t\t\t\t     3 years through strategic growth initi...'\n",
            "\u001b[91mFailed to find '3 years' near Para 31\u001b[0m\n",
            "   Context in Doc: 'Investment Diversification...'\n",
            "\u001b[91mFailed to find '8%' near Para 34\u001b[0m\n",
            "   Context in Doc: 'Early Retirement...'\n",
            "\u001b[91mFailed to find '£60,000' near Para 40\u001b[0m\n",
            "   Context in Doc: 'aai!\twww.advisoryai.com\t...'\n",
            "\u001b[91mFailed to find '55' near Para 40\u001b[0m\n",
            "   Context in Doc: 'aai!\twww.advisoryai.com\t...'\n",
            "\n",
            "\u001b[92mSuccess! Injected 40/45 tags.\u001b[0m\n",
            "Saved to: suitability_report_tagged.docx\n"
          ]
        }
      ],
      "source": [
        "!python injection.py suitability_report.docx suitability_report_analysis.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8QlF6S7Vn1C",
        "outputId": "fb992281-cc06-4288-c045-18cd41ffacaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting docxtpl\n",
            "  Downloading docxtpl-0.20.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (from docxtpl) (1.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from docxtpl) (3.1.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from docxtpl) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->docxtpl) (3.0.3)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx->docxtpl) (4.15.0)\n",
            "Downloading docxtpl-0.20.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: docxtpl\n",
            "Successfully installed docxtpl-0.20.2\n"
          ]
        }
      ],
      "source": [
        "%pip install docxtpl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYrLtYE6VkID",
        "outputId": "2614b2a0-70c4-414a-b559-3a945356d92d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting render.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile render.py\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "from docxtpl import DocxTemplate\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# MOCK DATA (In a real app, this comes from your Qdrant/Postgres DB)\n",
        "# -----------------------------------------------------------------------------\n",
        "MOCK_CLIENT_DATA = {\n",
        "    # 1. Simple Text Variables\n",
        "    \"client_first_name\": \"Bruce\",\n",
        "    \"client_name\": \"Bruce Wayne\",\n",
        "    \"client_full_name\": \"Mr. Bruce Wayne\",\n",
        "\n",
        "    # 2. Financial Goals (The text you tagged)\n",
        "    \"business_revenue_target_amount\": \"£10,000,000\",\n",
        "    \"business_revenue_target_timeline\": \"5 years\",\n",
        "    \"investment_target_annual_return\": \"12%\",\n",
        "    \"retirement_annual_income_goal\": \"£500,000\",\n",
        "    \"retirement_age_goal\": \"60\",\n",
        "\n",
        "    # 3. Table Data (Matching tags in your table)\n",
        "    \"asset_type\": \"Wayne Enterprises Stock\",\n",
        "    \"asset_business_value\": \"£5,000,000\",\n",
        "    \"asset_monthly_income\": \"£25,000\",\n",
        "    \"asset_current_debt\": \"£0\",\n",
        "    \"asset_growth_rate\": \"8%\",\n",
        "    \"asset_value\": \"£1,200,000\",\n",
        "    \"asset_monthly_contribution\": \"£5,000\",\n",
        "    \"asset_withdrawal\": \"£0\",\n",
        "\n",
        "    # 4. Investment Specifics\n",
        "    \"investment_type\": \"Global Tech Fund\",\n",
        "    \"investment_amount\": \"£2,000,000\",\n",
        "    \"current_value\": \"£2,400,000\",\n",
        "    \"expected_return_rate\": \"10%\",\n",
        "    \"asset_income\": \"£12,000\",\n",
        "\n",
        "    # 5. Emergency Fund\n",
        "    \"emergency_fund_name\": \"High Yield Cash\",\n",
        "    \"emergency_fund_current_value\": \"£100,000\",\n",
        "    \"emergency_fund_monthly_contribution\": \"£1,000\",\n",
        "    \"emergency_fund_target_value\": \"£500,000\",\n",
        "    \"emergency_fund_interest_rate\": \"4.5%\",\n",
        "    \"asset_current_value\": \"£50,000\"\n",
        "}\n",
        "\n",
        "def render_report(template_path, output_path):\n",
        "    if not os.path.exists(template_path):\n",
        "        print(f\"Error: Template not found at {template_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Rendering report for {MOCK_CLIENT_DATA['client_name']}...\")\n",
        "\n",
        "    try:\n",
        "        doc = DocxTemplate(template_path)\n",
        "\n",
        "        # The Magic: Jinja2 merges the data into the tags\n",
        "        doc.render(MOCK_CLIENT_DATA)\n",
        "\n",
        "        doc.save(output_path)\n",
        "        print(f\"✅ Success! Report generated: {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Render failed: {e}\")\n",
        "        # Hint for debugging Jinja errors\n",
        "        if \"undeclared variable\" in str(e):\n",
        "            print(\"Tip: A tag in the Docx doesn't match a key in MOCK_CLIENT_DATA.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python render.py <tagged_template.docx>\")\n",
        "    else:\n",
        "        # Auto-generate output filename\n",
        "        input_file = sys.argv[1]\n",
        "        output_file = input_file.replace(\".docx\", \"_FINAL.docx\")\n",
        "        render_report(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL-hS7lhVumr",
        "outputId": "c1aaca61-fdb7-4333-f50b-20fe28fb1a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering report for Bruce Wayne...\n",
            "✅ Success! Report generated: suitability_report_tagged_FINAL.docx\n"
          ]
        }
      ],
      "source": [
        "!python render.py suitability_report_tagged.docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3FxdxKPW-lU",
        "outputId": "f17caf92-6b13-4425-a248-e6cddf6c324c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing analyze_v2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile analyze_v2.py\n",
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import re\n",
        "from docx import Document\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# CONFIG\n",
        "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "MODEL = \"google/gemini-2.0-flash-exp:free\"\n",
        "MAX_CONCURRENT = 10\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a Financial Template Architect.\n",
        "Identify dynamic variables (Names, Dates, Amounts) in the text.\n",
        "Return JSON: { \"variables\": [{ \"original_text\": \"...\", \"suggested_tag\": \"...\", \"type\": \"...\" }] }\n",
        "\"\"\"\n",
        "\n",
        "def clean_json(content):\n",
        "    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)\n",
        "    content = re.sub(r'```json\\s*', '', content)\n",
        "    return content.replace('```', '').strip()\n",
        "\n",
        "def iter_all_content(doc):\n",
        "    \"\"\"Yields every piece of text in the document.\"\"\"\n",
        "    # Body\n",
        "    for i, p in enumerate(doc.paragraphs): yield p, f\"Body Para {i}\"\n",
        "    # Tables\n",
        "    for t_i, table in enumerate(doc.tables):\n",
        "        for r_i, row in enumerate(table.rows):\n",
        "            # Combine row text for context\n",
        "            row_text = \" | \".join([c.text.strip() for c in row.cells if c.text.strip()])\n",
        "            if row_text: yield row_text, f\"Table {t_i} Row {r_i}\"\n",
        "    # Headers/Footers\n",
        "    for section in doc.sections:\n",
        "        for h in [section.header, section.first_page_header]:\n",
        "            if h:\n",
        "                for i, p in enumerate(h.paragraphs): yield p, f\"Header Para {i}\"\n",
        "        for f in [section.footer, section.first_page_footer]:\n",
        "            if f:\n",
        "                for i, p in enumerate(f.paragraphs): yield p, f\"Footer Para {i}\"\n",
        "\n",
        "async def analyze_item(sem, client, text, source_label):\n",
        "    async with sem:\n",
        "        try:\n",
        "            # Skip empty or very short text\n",
        "            if len(text) < 3: return []\n",
        "\n",
        "            completion = await client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": f\"Context: {source_label}\\nText: '{text}'\"}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "            raw = completion.choices[0].message.content\n",
        "            data = json.loads(clean_json(raw))\n",
        "\n",
        "            # Add metadata\n",
        "            vars = data.get(\"variables\", [])\n",
        "            for v in vars:\n",
        "                v['source_label'] = source_label\n",
        "            return vars\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "async def main(file_path):\n",
        "    doc = Document(file_path)\n",
        "    client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL, default_headers={\"HTTP-Referer\": \"https://colab.research.google.com\"})\n",
        "\n",
        "    tasks = []\n",
        "    sem = asyncio.Semaphore(MAX_CONCURRENT)\n",
        "\n",
        "    print(f\"Scanning deep structure of {file_path}...\")\n",
        "\n",
        "    for content, label in iter_all_content(doc):\n",
        "        # Handle both Paragraph objects and raw strings (from tables)\n",
        "        text = content.text.strip() if hasattr(content, 'text') else str(content)\n",
        "        tasks.append(analyze_item(sem, client, text, label))\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    flat_results = [item for sublist in results for item in sublist]\n",
        "\n",
        "    out_file = file_path.replace(\".docx\", \"_analysis_v2.json\")\n",
        "    with open(out_file, \"w\") as f:\n",
        "        json.dump(flat_results, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Found {len(flat_results)} variables (including headers/footers).\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    asyncio.run(main(sys.argv[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-eN13UCqDPs",
        "outputId": "d1eefae5-f791-4cb2-97f8-3fe8e566084e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GrrsIWkqEyV",
        "outputId": "020b99a7-284c-4fde-9419-cff40edb0585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing normalizer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile normalizer.py\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from lxml import etree\n",
        "from copy import deepcopy\n",
        "\n",
        "# Namespaces are critical in WordML\n",
        "NS = {\n",
        "    'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main',\n",
        "    'xml': 'http://www.w3.org/XML/1998/namespace'\n",
        "}\n",
        "\n",
        "def clean_and_normalize_docx(input_path, output_path):\n",
        "    \"\"\"\n",
        "    1. Unzips docx\n",
        "    2. Parses word/document.xml\n",
        "    3. Removes \"noise\" nodes (proofErr, etc)\n",
        "    4. Merges adjacent runs with identical formatting\n",
        "    5. Rezips into a clean docx\n",
        "    \"\"\"\n",
        "    # 1. Setup temporary workspace\n",
        "    temp_dir = \"temp_docx_extract\"\n",
        "    if os.path.exists(temp_dir):\n",
        "        shutil.rmtree(temp_dir)\n",
        "    os.makedirs(temp_dir)\n",
        "\n",
        "    # 2. Unzip\n",
        "    with zipfile.ZipFile(input_path, 'r') as z:\n",
        "        z.extractall(temp_dir)\n",
        "\n",
        "    # 3. Process the main XML content\n",
        "    doc_xml_path = os.path.join(temp_dir, 'word', 'document.xml')\n",
        "    if not os.path.exists(doc_xml_path):\n",
        "        print(\"Error: Invalid docx (no document.xml found)\")\n",
        "        return\n",
        "\n",
        "    # Parse with lxml\n",
        "    tree = etree.parse(doc_xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # --- STEP A: SANITIZE (Remove Noise) ---\n",
        "    print(\"  - Sanitizing XML nodes...\")\n",
        "    remove_noise_nodes(root)\n",
        "\n",
        "    # --- STEP B: NORMALIZE (Merge Runs) ---\n",
        "    print(\"  - Merging broken runs...\")\n",
        "    merge_adjacent_runs(root)\n",
        "\n",
        "    # 4. Save modified XML\n",
        "    with open(doc_xml_path, 'wb') as f:\n",
        "        tree.write(f, xml_declaration=True, encoding='UTF-8', standalone=True)\n",
        "\n",
        "    # 5. Re-zip\n",
        "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as z:\n",
        "        for folder_name, subfolders, filenames in os.walk(temp_dir):\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(folder_name, filename)\n",
        "                # Compute path relative to temp_dir\n",
        "                arcname = os.path.relpath(file_path, temp_dir)\n",
        "                z.write(file_path, arcname)\n",
        "\n",
        "    # Cleanup\n",
        "    shutil.rmtree(temp_dir)\n",
        "    print(f\"✅ Normalized document saved to: {output_path}\")\n",
        "\n",
        "def remove_noise_nodes(root):\n",
        "    \"\"\"\n",
        "    Removes <w:proofErr>, <w:lastRenderedPageBreak>, and clears RSID attributes.\n",
        "    These are the primary causes of split text.\n",
        "    \"\"\"\n",
        "    # 1. Remove specific tags that split text\n",
        "    tags_to_remove = [\n",
        "        f\"{{{NS['w']}}}proofErr\",             # Spell check markers\n",
        "        f\"{{{NS['w']}}}lastRenderedPageBreak\", # Page break markers\n",
        "        f\"{{{NS['w']}}}noProof\",              # Grammar skip markers\n",
        "        f\"{{{NS['w']}}}lang\"                  # Language tags (often cause splits)\n",
        "    ]\n",
        "\n",
        "    for tag in tags_to_remove:\n",
        "        for element in root.findall(f\".//{tag}\", NS):\n",
        "            # Remove the element but keep its tail text if any (rare for these tags)\n",
        "            parent = element.getparent()\n",
        "            if parent is not None:\n",
        "                parent.remove(element)\n",
        "\n",
        "    # 2. Strip RSID attributes (Revision Save IDs) from ALL elements\n",
        "    # These change every time you edit the doc, splitting runs needlessly.\n",
        "    for elem in root.iter():\n",
        "        for attrib in list(elem.attrib):\n",
        "            if attrib.endswith('rsidR') or attrib.endswith('rsidRPr') or attrib.endswith('rsidRDefault'):\n",
        "                del elem.attrib[attrib]\n",
        "\n",
        "def merge_adjacent_runs(root):\n",
        "    \"\"\"\n",
        "    The Core Logic: Greedy Merge.\n",
        "    Iterates through paragraphs. If Run A and Run B are neighbors\n",
        "    and have IDENTICAL properties (w:rPr), merge them.\n",
        "    \"\"\"\n",
        "    paragraphs = root.findall(f\".//{{{NS['w']}}}p\", NS)\n",
        "\n",
        "    for p in paragraphs:\n",
        "        children = list(p)\n",
        "        if not children: continue\n",
        "\n",
        "        i = 0\n",
        "        while i < len(children) - 1:\n",
        "            current_node = children[i]\n",
        "            next_node = children[i+1]\n",
        "\n",
        "            # Check if both are Runs (<w:r>)\n",
        "            if current_node.tag == f\"{{{NS['w']}}}r\" and next_node.tag == f\"{{{NS['w']}}}r\":\n",
        "\n",
        "                # Get Properties (w:rPr)\n",
        "                curr_props = current_node.find(f\"{{{NS['w']}}}rPr\", NS)\n",
        "                next_props = next_node.find(f\"{{{NS['w']}}}rPr\", NS)\n",
        "\n",
        "                # Compare Properties using Canonical XML String\n",
        "                # (This ensures <b/><i/> equals <b/><i/> regardless of whitespace)\n",
        "                curr_props_str = etree.tostring(curr_props, method=\"c14n\") if curr_props is not None else b\"\"\n",
        "                next_props_str = etree.tostring(next_props, method=\"c14n\") if next_props is not None else b\"\"\n",
        "\n",
        "                if curr_props_str == next_props_str:\n",
        "                    # MERGE!\n",
        "\n",
        "                    # 1. Get text elements\n",
        "                    curr_t = current_node.find(f\"{{{NS['w']}}}t\", NS)\n",
        "                    next_t = next_node.find(f\"{{{NS['w']}}}t\", NS)\n",
        "\n",
        "                    if curr_t is not None and next_t is not None:\n",
        "                        # Append text\n",
        "                        text_val = (curr_t.text or \"\") + (next_t.text or \"\")\n",
        "                        curr_t.text = text_val\n",
        "\n",
        "                        # Preserve space attribute if either had it\n",
        "                        space_attr = f\"{{{NS['xml']}}}space\"\n",
        "                        if next_t.get(space_attr) == 'preserve':\n",
        "                            curr_t.set(space_attr, 'preserve')\n",
        "\n",
        "                        # Remove the consumed 'next_node'\n",
        "                        p.remove(next_node)\n",
        "\n",
        "                        # Update children list and stay at index 'i' to try merging next one\n",
        "                        children = list(p)\n",
        "                        continue\n",
        "\n",
        "            # Move to next node\n",
        "            i += 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python normalizer.py input.docx\")\n",
        "    else:\n",
        "        input_file = sys.argv[1]\n",
        "        output_file = input_file.replace(\".docx\", \"_clean.docx\")\n",
        "        clean_and_normalize_docx(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxQXBQM7qUZZ",
        "outputId": "bbf47c6d-1302-4aff-87fb-e3afa1d7d3c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - Sanitizing XML nodes...\n",
            "  - Merging broken runs...\n",
            "✅ Normalized document saved to: suitability_report_clean.docx\n"
          ]
        }
      ],
      "source": [
        "!python normalizer.py suitability_report.docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv_wV9MTrFsq",
        "outputId": "3001ac21-7705-4da7-c339-7ed1b1cb4c3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning deep structure of suitability_report.docx...\n",
            "✅ Found 27 logic points.\n",
            "Saved to suitability_report_analysis_v2.json\n"
          ]
        }
      ],
      "source": [
        "!python analyze_v2.py suitability_report.docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ype0nShsrjew",
        "outputId": "2ed1bd33-14dd-44d9-ec98-0bfb2c738ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting analyze_v2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile analyze_v2.py\n",
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "from docx import Document\n",
        "from openai import AsyncOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "MODEL = \"google/gemini-2.0-flash-exp:free\"\n",
        "MAX_CONCURRENT_REQUESTS = 10\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PYDANTIC SCHEMAS (STRUCTURED OUTPUT)\n",
        "# -----------------------------------------------------------------------------\n",
        "class Variable(BaseModel):\n",
        "    original_text: str\n",
        "    suggested_tag: str\n",
        "    type: str = Field(..., description=\"text, money, date, or percentage\")\n",
        "\n",
        "class TableLoop(BaseModel):\n",
        "    is_loop: bool\n",
        "    list_variable: Optional[str] = Field(None, description=\"e.g. 'pensions', 'assets'\")\n",
        "    item_variable: Optional[str] = Field(None, description=\"e.g. 'item'\")\n",
        "    columns_mapping: Optional[dict] = Field(None, description=\"{ '£50,000': 'item.value' }\")\n",
        "\n",
        "class Conditional(BaseModel):\n",
        "    is_conditional: bool\n",
        "    condition_expression: Optional[str] = Field(None, description=\"Jinja2 expression e.g. 'client.has_mortgage'\")\n",
        "\n",
        "class AnalysisResult(BaseModel):\n",
        "    variables: List[Variable] = []\n",
        "    table_loop: Optional[TableLoop] = None\n",
        "    conditional: Optional[Conditional] = None\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a Document Architect. Analyze the text context.\n",
        "1. Identify DYNAMIC VARIABLES (Names, Dates).\n",
        "2. Detect LOOPS in tables (e.g., if a row looks like a list item).\n",
        "3. Detect CONDITIONAL paragraphs (e.g., advice that only applies to specific clients).\n",
        "\n",
        "Output valid JSON matching the schema.\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# ANALYSIS LOGIC\n",
        "# -----------------------------------------------------------------------------\n",
        "async def analyze_segment(sem, client, text, context_label, metadata):\n",
        "    async with sem:\n",
        "        if len(text) < 3: return {}\n",
        "        try:\n",
        "            completion = await client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": f\"Context: {context_label}\\nText: '{text}'\"}\n",
        "                ],\n",
        "                temperature=0.1,\n",
        "                response_format={\"type\": \"json_object\"}\n",
        "            )\n",
        "            # Parse JSON\n",
        "            raw = completion.choices[0].message.content\n",
        "            # Clean generic markdown if present\n",
        "            raw = raw.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
        "            data = json.loads(raw)\n",
        "\n",
        "            # Merge Metadata\n",
        "            data['metadata'] = metadata\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            # print(f\"Error: {e}\")\n",
        "            return {}\n",
        "\n",
        "def iter_all_content(doc):\n",
        "    \"\"\"Yields Body, Tables, Headers, Footers.\"\"\"\n",
        "    # Body Paragraphs\n",
        "    for i, p in enumerate(doc.paragraphs):\n",
        "        yield p.text.strip(), f\"Body Para {i}\", {\"type\": \"paragraph\", \"index\": i}\n",
        "\n",
        "    # Body Tables\n",
        "    for t_i, table in enumerate(doc.tables):\n",
        "        for r_i, row in enumerate(table.rows):\n",
        "            # Skip header row usually\n",
        "            if r_i == 0: continue\n",
        "            text = \" | \".join([c.text.strip() for c in row.cells if c.text.strip()])\n",
        "            if text:\n",
        "                yield text, f\"Table {t_i} Row {r_i}\", {\"type\": \"table_row\", \"table_index\": t_i, \"row_index\": r_i}\n",
        "\n",
        "async def main(file_path):\n",
        "    doc = Document(file_path)\n",
        "    client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL, default_headers={\"HTTP-Referer\": \"https://colab.research.google.com\"})\n",
        "\n",
        "    sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "    tasks = []\n",
        "\n",
        "    print(f\"Scanning deep structure of {file_path}...\")\n",
        "\n",
        "    for text, label, meta in iter_all_content(doc):\n",
        "        tasks.append(analyze_segment(sem, client, text, label, meta))\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Filter empty results\n",
        "    clean_results = [r for r in results if r]\n",
        "\n",
        "    out_file = file_path.replace(\".docx\", \"_analysis_v2.json\")\n",
        "    with open(out_file, \"w\") as f:\n",
        "        json.dump(clean_results, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Found {len(clean_results)} logic points.\")\n",
        "    print(f\"Saved to {out_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    asyncio.run(main(sys.argv[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2otY0SOrPkL",
        "outputId": "2b6288fd-9a7d-4825-cd0e-e9c94d58e6da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing render_v2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile render_v2.py\n",
        "import sys\n",
        "from docxtpl import DocxTemplate\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# MOCK DATA - This is what your \"Database\" would return\n",
        "# -------------------------------------------------------\n",
        "CONTEXT = {\n",
        "    # 1. Basic Info\n",
        "    \"client_name\": \"Bruce Wayne\",\n",
        "    \"report_date\": \"February 3, 2026\",\n",
        "\n",
        "    # 2. Conditionals (Try changing these to False to see sections vanish!)\n",
        "    \"client\": {\n",
        "        \"has_mortgage\": True,\n",
        "        \"is_high_risk\": False,\n",
        "        \"is_retiring_soon\": True\n",
        "    },\n",
        "\n",
        "    # 3. LOOPS (The Table Data)\n",
        "    # The template expects 'pensions' or 'assets'.\n",
        "    # Ensure this key matches what the Analyzer found in step 2.\n",
        "    \"pensions\": [\n",
        "        {\"provider\": \"Wayne Corp Life\", \"value\": \"£1,500,000\", \"fee\": \"0.1%\"},\n",
        "        {\"provider\": \"Gotham City Fund\", \"value\": \"£50,000\", \"fee\": \"0.5%\"},\n",
        "        {\"provider\": \"Alfred Savings\", \"value\": \"£10,000\", \"fee\": \"0.0%\"}\n",
        "    ],\n",
        "\n",
        "    \"assets\": [\n",
        "        {\"name\": \"Batcave\", \"value\": \"£100m\"},\n",
        "        {\"name\": \"Manor\", \"value\": \"£50m\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "def render(template_path):\n",
        "    print(f\"Rendering {template_path} with rich data...\")\n",
        "    doc = DocxTemplate(template_path)\n",
        "\n",
        "    try:\n",
        "        doc.render(CONTEXT)\n",
        "        out_path = template_path.replace(\".docx\", \"_FINAL_RENDER.docx\")\n",
        "        doc.save(out_path)\n",
        "        print(f\"✅ Success! Generated: {out_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Render Error: {e}\")\n",
        "        print(\"Tip: Check if the variable names in CONTEXT match the tags in the doc.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "        render(sys.argv[1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

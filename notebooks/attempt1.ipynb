{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KBV89vMewKWV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBhpIWM7qDiq",
        "outputId": "c044f496-dbe8-43f5-9b2a-0dbb6fce7cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.16.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: python-docx, colorama\n",
            "Successfully installed colorama-0.4.6 python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "%pip install python-docx openai pydantic colorama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zaBPmb2rOUw",
        "outputId": "ceba4aca-cb65-4eba-d108-f49ac778fdde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sk-or-v1-\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ------------------------------------------------------------------\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-or-v1-\"\n",
        "os.environ[\"OPENAI_BASE_URL\"] = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "print(os.environ[\"OPENAI_API_KEY\"])\n",
        "# Choose your model. For hackathons, Claude 3.5 Sonnet is amazing at code/structure.\n",
        "# OpenRouter Model ID: \"anthropic/claude-3.5-sonnet\" or \"openai/gpt-4o\"\n",
        "MODEL_NAME = \"tngtech/deepseek-r1t2-chimera:free\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DMcLxiHXviWW"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyslxtD7vj5v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Paste your actual key here\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqnqjKe7sGYh",
        "outputId": "2d4d9ef9-994f-43c4-ed5c-ff99e6671981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning deep structure of suitability_report.docx...\n",
            "✅ Found 0 variables (including headers/footers).\n"
          ]
        }
      ],
      "source": [
        "!python analyze_v2.py suitability_report.docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HflOLkOtz5H",
        "outputId": "1695b50a-b1d1-4eb4-b412-081df8096caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting analyze.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile analyze.py\n",
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import re\n",
        "from docx import Document\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "MODEL = \"google/gemini-2.5-flash\"\n",
        "\n",
        "# Concurrency Limit\n",
        "MAX_CONCURRENT_REQUESTS = 10\n",
        "\n",
        "ANALYSIS_SYSTEM_PROMPT = \"\"\"\n",
        "You are a Financial Document Expert. Analyze the text segment to identify \"Template Variables\".\n",
        "\n",
        "LOOK FOR:\n",
        "1. **Client Details:** Name (e.g., Sarah), Age.\n",
        "2. **Financial Goals:** specific amounts (£500k), dates (3 years), rates (8%).\n",
        "3. **Line Items:** If the text is a row in a table (e.g. \"Business Capital | £750,000\"), extract the values.\n",
        "\n",
        "OUTPUT JSON FORMAT:\n",
        "{\n",
        "  \"variables\": [\n",
        "    { \"original_text\": \"Sarah\", \"suggested_tag\": \"client_first_name\", \"type\": \"text\" },\n",
        "    { \"original_text\": \"£750,000\", \"suggested_tag\": \"asset_business_value\", \"type\": \"money\" }\n",
        "  ]\n",
        "}\n",
        "If no variables found, return { \"variables\": [] }\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. HELPER FUNCTIONS\n",
        "# -----------------------------------------------------------------------------\n",
        "def clean_json_response(content):\n",
        "    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)\n",
        "    content = re.sub(r'```json\\s*', '', content)\n",
        "    content = re.sub(r'```\\s*', '', content)\n",
        "    return content.strip()\n",
        "\n",
        "async def analyze_segment(sem, client, text, context_label, metadata):\n",
        "    \"\"\"\n",
        "    Analyzes a text segment (paragraph or table row).\n",
        "    \"\"\"\n",
        "    async with sem:\n",
        "        try:\n",
        "            # We provide context to the LLM (\"This is a Table Row\")\n",
        "            prompt = f\"Context: {context_label}\\nAnalyze this text: '{text}'\"\n",
        "\n",
        "            completion = await client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": ANALYSIS_SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            cleaned = clean_json_response(completion.choices[0].message.content)\n",
        "\n",
        "            try:\n",
        "                data = json.loads(cleaned)\n",
        "                variables = data.get(\"variables\", [])\n",
        "            except json.JSONDecodeError:\n",
        "                start = cleaned.find('{')\n",
        "                end = cleaned.rfind('}') + 1\n",
        "                if start != -1 and end != -1:\n",
        "                    variables = json.loads(cleaned[start:end]).get(\"variables\", [])\n",
        "                else:\n",
        "                    variables = []\n",
        "\n",
        "            # Enrich with location metadata\n",
        "            for v in variables:\n",
        "                v.update(metadata)\n",
        "\n",
        "            return variables\n",
        "\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. MAIN LOGIC\n",
        "# -----------------------------------------------------------------------------\n",
        "async def analyze_document(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return\n",
        "\n",
        "    doc = Document(file_path)\n",
        "    client = AsyncOpenAI(\n",
        "        api_key=API_KEY,\n",
        "        base_url=BASE_URL,\n",
        "        default_headers={\"HTTP-Referer\": \"https://colab.research.google.com\"}\n",
        "    )\n",
        "\n",
        "    print(f\"Analyzing {file_path}...\")\n",
        "    tasks = []\n",
        "    sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
        "\n",
        "    # --- A. SCAN PARAGRAPHS (With Context Window) ---\n",
        "    print(\"Scanning Paragraphs...\")\n",
        "    prev_text = \"\"\n",
        "    for i, para in enumerate(doc.paragraphs):\n",
        "        text = para.text.strip()\n",
        "\n",
        "        # Lower threshold to catch \"Sarah\" or \"Age 55\"\n",
        "        if len(text) < 3:\n",
        "            continue\n",
        "\n",
        "        # Create a \"Context Window\" (Previous Para + Current Para)\n",
        "        # This helps the AI know that \"Sarah\" is a Name.\n",
        "        combined_input = f\"Previous Line: {prev_text}\\nCurrent Line: {text}\"\n",
        "\n",
        "        tasks.append(analyze_segment(\n",
        "            sem, client, combined_input, \"Paragraph\",\n",
        "            {\"location_type\": \"paragraph\", \"index\": i}\n",
        "        ))\n",
        "\n",
        "        prev_text = text if len(text) > 5 else prev_text\n",
        "\n",
        "    # --- B. SCAN TABLES (Crucial for Financial Docs) ---\n",
        "    print(f\"Scanning {len(doc.tables)} Tables...\")\n",
        "    for t_idx, table in enumerate(doc.tables):\n",
        "        for r_idx, row in enumerate(table.rows):\n",
        "            # Convert row to a CSV-like string for the LLM\n",
        "            # e.g., \"Business Capital | Sarah | £750,000 | 15%\"\n",
        "            cells = [c.text.strip() for c in row.cells if c.text.strip()]\n",
        "            if not cells: continue\n",
        "\n",
        "            row_text = \" | \".join(cells)\n",
        "\n",
        "            # Skip header rows (heuristic: usually first row)\n",
        "            if r_idx == 0 and len(table.rows) > 1:\n",
        "                # Optional: You can choose to skip or scan headers.\n",
        "                # Scaning headers might help context but usually isn't data.\n",
        "                pass\n",
        "\n",
        "            tasks.append(analyze_segment(\n",
        "                sem, client, row_text, f\"Table {t_idx} Row {r_idx}\",\n",
        "                {\"location_type\": \"table\", \"table_index\": t_idx, \"row_index\": r_idx}\n",
        "            ))\n",
        "\n",
        "    # --- C. EXECUTE & SAVE ---\n",
        "    print(f\"Processing {len(tasks)} segments...\")\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    all_detections = [item for sublist in results for item in sublist]\n",
        "\n",
        "    out_file = file_path.replace(\".docx\", \"_analysis.json\")\n",
        "    with open(out_file, \"w\") as f:\n",
        "        json.dump(all_detections, f, indent=2)\n",
        "\n",
        "    print(f\"\\n✅ Found {len(all_detections)} variables (Clients, Assets, Liabilities).\")\n",
        "    print(f\"Saved to {out_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    asyncio.run(analyze_document(sys.argv[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIFvRYy6Qdw1",
        "outputId": "f1024332-e880-4ef2-b98c-649f529420de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting injection.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile injection.py\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "from docx import Document\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normalizes text to handle smart quotes and non-breaking spaces.\"\"\"\n",
        "    if not text: return \"\"\n",
        "    return text.replace('\\u00a0', ' ').replace('’', \"'\").replace('“', '\"').replace('”', '\"').strip()\n",
        "\n",
        "def inject_tags(docx_path, json_path):\n",
        "    if not os.path.exists(docx_path):\n",
        "        print(f\"{RED}Error: Document not found at {docx_path}{RESET}\")\n",
        "        return\n",
        "    if not os.path.exists(json_path):\n",
        "        print(f\"{RED}Error: JSON analysis not found at {json_path}{RESET}\")\n",
        "        return\n",
        "\n",
        "    doc = Document(docx_path)\n",
        "    with open(json_path, \"r\") as f:\n",
        "        variables = json.load(f)\n",
        "\n",
        "    print(f\"Injecting {len(variables)} tags into {docx_path}...\")\n",
        "    success_count = 0\n",
        "\n",
        "    for var in variables:\n",
        "        original_text = var.get('original_text')\n",
        "        tag_name = var.get('suggested_tag')\n",
        "        loc_type = var.get('location_type', 'paragraph')\n",
        "        target_index = var.get('index')\n",
        "\n",
        "        if not original_text or not tag_name:\n",
        "            continue\n",
        "\n",
        "        # Create the Tag\n",
        "        new_tag = f\"{{{{ {tag_name} }}}}\"\n",
        "\n",
        "        # -------------------------------------------------------\n",
        "        # STRATEGY 1: PARAGRAPH (With Search Window)\n",
        "        # -------------------------------------------------------\n",
        "        if loc_type == 'paragraph':\n",
        "            found = False\n",
        "\n",
        "            # Check the exact index first\n",
        "            if target_index < len(doc.paragraphs):\n",
        "                para = doc.paragraphs[target_index]\n",
        "                if smart_replace(para, original_text, new_tag):\n",
        "                    success_count += 1\n",
        "                    found = True\n",
        "\n",
        "            # If not found, check neighbors (Fuzzy Search)\n",
        "            # This handles cases where paragraph counts shift slightly\n",
        "            if not found:\n",
        "                start = max(0, target_index - 2)\n",
        "                end = min(len(doc.paragraphs), target_index + 3)\n",
        "                for i in range(start, end):\n",
        "                    if i == target_index: continue # Already checked\n",
        "                    if smart_replace(doc.paragraphs[i], original_text, new_tag):\n",
        "                        print(f\"{YELLOW}  -> Auto-corrected index from {target_index} to {i}{RESET}\")\n",
        "                        success_count += 1\n",
        "                        found = True\n",
        "                        break\n",
        "\n",
        "            if not found:\n",
        "                # DEBUG OUTPUT: Show user what went wrong\n",
        "                print(f\"{RED}Failed to find '{original_text}' near Para {target_index}{RESET}\")\n",
        "                if target_index < len(doc.paragraphs):\n",
        "                    print(f\"   Context in Doc: '{doc.paragraphs[target_index].text[:50]}...'\")\n",
        "\n",
        "        # -------------------------------------------------------\n",
        "        # STRATEGY 2: TABLE INJECTION\n",
        "        # -------------------------------------------------------\n",
        "        elif loc_type == 'table':\n",
        "            try:\n",
        "                table_idx = var.get('table_index')\n",
        "                row_idx = var.get('row_index')\n",
        "                if table_idx is not None and row_idx is not None:\n",
        "                    row = doc.tables[table_idx].rows[row_idx]\n",
        "                    for cell in row.cells:\n",
        "                        for p in cell.paragraphs:\n",
        "                            if smart_replace(p, original_text, new_tag):\n",
        "                                success_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"{RED}Table error: {e}{RESET}\")\n",
        "\n",
        "    output_path = docx_path.replace(\".docx\", \"_tagged.docx\")\n",
        "    doc.save(output_path)\n",
        "    print(f\"\\n{GREEN}Success! Injected {success_count}/{len(variables)} tags.{RESET}\")\n",
        "    print(f\"Saved to: {output_path}\")\n",
        "\n",
        "def smart_replace(paragraph, search_text, replace_text):\n",
        "    \"\"\"\n",
        "    Attempts to replace text handling 'runs' and 'normalization'.\n",
        "    \"\"\"\n",
        "    # 1. Normalize both sides for comparison\n",
        "    clean_para = normalize_text(paragraph.text)\n",
        "    clean_search = normalize_text(search_text)\n",
        "\n",
        "    # 2. Check if text is even present\n",
        "    if clean_search not in clean_para:\n",
        "        return False\n",
        "\n",
        "    # 3. Try Run Replacement (Preserves Bold/Color)\n",
        "    for run in paragraph.runs:\n",
        "        if search_text in run.text:\n",
        "            run.text = run.text.replace(search_text, replace_text)\n",
        "            return True\n",
        "\n",
        "    # 4. Fallback: Direct Replace (Might lose some styling, but ensures tag is inserted)\n",
        "    # We use the raw text replace here\n",
        "    if search_text in paragraph.text:\n",
        "        paragraph.text = paragraph.text.replace(search_text, replace_text)\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 3:\n",
        "        print(\"Usage: python injection.py <doc.docx> <analysis.json>\")\n",
        "    else:\n",
        "        inject_tags(sys.argv[1], sys.argv[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6_aPA3eQyl_",
        "outputId": "8357062d-83a3-468b-f340-60de87c8848d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Injecting 45 tags into suitability_report.docx...\n",
            "\u001b[91mFailed to find '£500,000' near Para 29\u001b[0m\n",
            "   Context in Doc: '   \t\t\t\t     3 years through strategic growth initi...'\n",
            "\u001b[91mFailed to find '3 years' near Para 31\u001b[0m\n",
            "   Context in Doc: 'Investment Diversification...'\n",
            "\u001b[91mFailed to find '8%' near Para 34\u001b[0m\n",
            "   Context in Doc: 'Early Retirement...'\n",
            "\u001b[91mFailed to find '£60,000' near Para 40\u001b[0m\n",
            "   Context in Doc: 'aai!\twww.advisoryai.com\t...'\n",
            "\u001b[91mFailed to find '55' near Para 40\u001b[0m\n",
            "   Context in Doc: 'aai!\twww.advisoryai.com\t...'\n",
            "\n",
            "\u001b[92mSuccess! Injected 40/45 tags.\u001b[0m\n",
            "Saved to: suitability_report_tagged.docx\n"
          ]
        }
      ],
      "source": [
        "!python injection.py suitability_report.docx suitability_report_analysis.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8QlF6S7Vn1C",
        "outputId": "fb992281-cc06-4288-c045-18cd41ffacaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting docxtpl\n",
            "  Downloading docxtpl-0.20.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (from docxtpl) (1.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from docxtpl) (3.1.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from docxtpl) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->docxtpl) (3.0.3)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx->docxtpl) (4.15.0)\n",
            "Downloading docxtpl-0.20.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: docxtpl\n",
            "Successfully installed docxtpl-0.20.2\n"
          ]
        }
      ],
      "source": [
        "%pip install docxtpl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYrLtYE6VkID",
        "outputId": "2614b2a0-70c4-414a-b559-3a945356d92d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting render.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile render.py\n",
        "import os\n",
        "import json\n",
        "import sys\n",
        "from docxtpl import DocxTemplate\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# MOCK DATA (In a real app, this comes from your Qdrant/Postgres DB)\n",
        "# -----------------------------------------------------------------------------\n",
        "MOCK_CLIENT_DATA = {\n",
        "    # 1. Simple Text Variables\n",
        "    \"client_first_name\": \"Bruce\",\n",
        "    \"client_name\": \"Bruce Wayne\",\n",
        "    \"client_full_name\": \"Mr. Bruce Wayne\",\n",
        "\n",
        "    # 2. Financial Goals (The text you tagged)\n",
        "    \"business_revenue_target_amount\": \"£10,000,000\",\n",
        "    \"business_revenue_target_timeline\": \"5 years\",\n",
        "    \"investment_target_annual_return\": \"12%\",\n",
        "    \"retirement_annual_income_goal\": \"£500,000\",\n",
        "    \"retirement_age_goal\": \"60\",\n",
        "\n",
        "    # 3. Table Data (Matching tags in your table)\n",
        "    \"asset_type\": \"Wayne Enterprises Stock\",\n",
        "    \"asset_business_value\": \"£5,000,000\",\n",
        "    \"asset_monthly_income\": \"£25,000\",\n",
        "    \"asset_current_debt\": \"£0\",\n",
        "    \"asset_growth_rate\": \"8%\",\n",
        "    \"asset_value\": \"£1,200,000\",\n",
        "    \"asset_monthly_contribution\": \"£5,000\",\n",
        "    \"asset_withdrawal\": \"£0\",\n",
        "\n",
        "    # 4. Investment Specifics\n",
        "    \"investment_type\": \"Global Tech Fund\",\n",
        "    \"investment_amount\": \"£2,000,000\",\n",
        "    \"current_value\": \"£2,400,000\",\n",
        "    \"expected_return_rate\": \"10%\",\n",
        "    \"asset_income\": \"£12,000\",\n",
        "\n",
        "    # 5. Emergency Fund\n",
        "    \"emergency_fund_name\": \"High Yield Cash\",\n",
        "    \"emergency_fund_current_value\": \"£100,000\",\n",
        "    \"emergency_fund_monthly_contribution\": \"£1,000\",\n",
        "    \"emergency_fund_target_value\": \"£500,000\",\n",
        "    \"emergency_fund_interest_rate\": \"4.5%\",\n",
        "    \"asset_current_value\": \"£50,000\"\n",
        "}\n",
        "\n",
        "def render_report(template_path, output_path):\n",
        "    if not os.path.exists(template_path):\n",
        "        print(f\"Error: Template not found at {template_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Rendering report for {MOCK_CLIENT_DATA['client_name']}...\")\n",
        "\n",
        "    try:\n",
        "        doc = DocxTemplate(template_path)\n",
        "\n",
        "        # The Magic: Jinja2 merges the data into the tags\n",
        "        doc.render(MOCK_CLIENT_DATA)\n",
        "\n",
        "        doc.save(output_path)\n",
        "        print(f\"✅ Success! Report generated: {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Render failed: {e}\")\n",
        "        # Hint for debugging Jinja errors\n",
        "        if \"undeclared variable\" in str(e):\n",
        "            print(\"Tip: A tag in the Docx doesn't match a key in MOCK_CLIENT_DATA.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python render.py <tagged_template.docx>\")\n",
        "    else:\n",
        "        # Auto-generate output filename\n",
        "        input_file = sys.argv[1]\n",
        "        output_file = input_file.replace(\".docx\", \"_FINAL.docx\")\n",
        "        render_report(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL-hS7lhVumr",
        "outputId": "c1aaca61-fdb7-4333-f50b-20fe28fb1a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendering report for Bruce Wayne...\n",
            "✅ Success! Report generated: suitability_report_tagged_FINAL.docx\n"
          ]
        }
      ],
      "source": [
        "!python render.py suitability_report_tagged.docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3FxdxKPW-lU",
        "outputId": "7934ee02-7984-4160-aee5-cc4faf79e298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing analyze_v2.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile analyze_v2.py\n",
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import re\n",
        "from docx import Document\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "# CONFIG\n",
        "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "MODEL = \"google/gemini-2.0-flash-exp:free\"\n",
        "MAX_CONCURRENT = 10\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a Financial Template Architect.\n",
        "Identify dynamic variables (Names, Dates, Amounts) in the text.\n",
        "Return JSON: { \"variables\": [{ \"original_text\": \"...\", \"suggested_tag\": \"...\", \"type\": \"...\" }] }\n",
        "\"\"\"\n",
        "\n",
        "def clean_json(content):\n",
        "    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)\n",
        "    content = re.sub(r'```json\\s*', '', content)\n",
        "    return content.replace('```', '').strip()\n",
        "\n",
        "def iter_all_content(doc):\n",
        "    \"\"\"Yields every piece of text in the document.\"\"\"\n",
        "    # Body\n",
        "    for i, p in enumerate(doc.paragraphs): yield p, f\"Body Para {i}\"\n",
        "    # Tables\n",
        "    for t_i, table in enumerate(doc.tables):\n",
        "        for r_i, row in enumerate(table.rows):\n",
        "            # Combine row text for context\n",
        "            row_text = \" | \".join([c.text.strip() for c in row.cells if c.text.strip()])\n",
        "            if row_text: yield row_text, f\"Table {t_i} Row {r_i}\"\n",
        "    # Headers/Footers\n",
        "    for section in doc.sections:\n",
        "        for h in [section.header, section.first_page_header]:\n",
        "            if h:\n",
        "                for i, p in enumerate(h.paragraphs): yield p, f\"Header Para {i}\"\n",
        "        for f in [section.footer, section.first_page_footer]:\n",
        "            if f:\n",
        "                for i, p in enumerate(f.paragraphs): yield p, f\"Footer Para {i}\"\n",
        "\n",
        "async def analyze_item(sem, client, text, source_label):\n",
        "    async with sem:\n",
        "        try:\n",
        "            # Skip empty or very short text\n",
        "            if len(text) < 3: return []\n",
        "\n",
        "            completion = await client.chat.completions.create(\n",
        "                model=MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": f\"Context: {source_label}\\nText: '{text}'\"}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "            raw = completion.choices[0].message.content\n",
        "            data = json.loads(clean_json(raw))\n",
        "\n",
        "            # Add metadata\n",
        "            vars = data.get(\"variables\", [])\n",
        "            for v in vars:\n",
        "                v['source_label'] = source_label\n",
        "            return vars\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "async def main(file_path):\n",
        "    doc = Document(file_path)\n",
        "    client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL, default_headers={\"HTTP-Referer\": \"https://colab.research.google.com\"})\n",
        "\n",
        "    tasks = []\n",
        "    sem = asyncio.Semaphore(MAX_CONCURRENT)\n",
        "\n",
        "    print(f\"Scanning deep structure of {file_path}...\")\n",
        "\n",
        "    for content, label in iter_all_content(doc):\n",
        "        # Handle both Paragraph objects and raw strings (from tables)\n",
        "        text = content.text.strip() if hasattr(content, 'text') else str(content)\n",
        "        tasks.append(analyze_item(sem, client, text, label))\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "    flat_results = [item for sublist in results for item in sublist]\n",
        "\n",
        "    out_file = file_path.replace(\".docx\", \"_analysis_v2.json\")\n",
        "    with open(out_file, \"w\") as f:\n",
        "        json.dump(flat_results, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Found {len(flat_results)} variables (including headers/footers).\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    asyncio.run(main(sys.argv[1]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
